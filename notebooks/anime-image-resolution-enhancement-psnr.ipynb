{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-28T14:54:48.313943Z",
     "iopub.status.busy": "2025-05-28T14:54:48.313460Z",
     "iopub.status.idle": "2025-05-28T14:55:22.207673Z",
     "shell.execute_reply": "2025-05-28T14:55:22.206730Z",
     "shell.execute_reply.started": "2025-05-28T14:54:48.313914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/anime-image-resolution-enhancement-psnr/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:55:22.209489Z",
     "iopub.status.busy": "2025-05-28T14:55:22.209262Z",
     "iopub.status.idle": "2025-05-28T14:59:44.299212Z",
     "shell.execute_reply": "2025-05-28T14:59:44.298421Z",
     "shell.execute_reply.started": "2025-05-28T14:55:22.209467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Real-ESRGAN\n",
      "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m72.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m322.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Requirements should be satisfied by a PEP 517 installer.\n",
      "        If you are using pip, you can try `pip install --use-pep517`.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  dist.fetch_build_eggs(dist.setup_requires)\n",
      "/usr/local/lib/python3.11/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  easy_install.initialize_options(self)\n",
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m411.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytorch-lightning 2.5.1.post0 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "# or\n",
    "#!git clone https://github.com/danhtran2mind/Real-ESRGAN.git\n",
    "%cd Real-ESRGAN\n",
    "!pip install --use-pep517 -q\n",
    "!pip install basicsr==1.4.2 -q\n",
    "!pip install facexlib -q\n",
    "!pip install gfpgan -q\n",
    "!pip install -r requirements.txt -q\n",
    "!python setup.py develop --quiet\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 -q\n",
    "!pip install numpy==1.26.4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:59:44.300935Z",
     "iopub.status.busy": "2025-05-28T14:59:44.300363Z",
     "iopub.status.idle": "2025-05-28T14:59:46.751076Z",
     "shell.execute_reply": "2025-05-28T14:59:46.750556Z",
     "shell.execute_reply.started": "2025-05-28T14:59:44.300907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import subprocess\n",
    "import torch\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:59:46.752910Z",
     "iopub.status.busy": "2025-05-28T14:59:46.752626Z",
     "iopub.status.idle": "2025-05-28T14:59:47.307182Z",
     "shell.execute_reply": "2025-05-28T14:59:47.306660Z",
     "shell.execute_reply.started": "2025-05-28T14:59:46.752885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, login\n",
    "login(\"<your_huggingface_token>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:00:50.298038Z",
     "iopub.status.busy": "2025-05-28T15:00:50.297439Z",
     "iopub.status.idle": "2025-05-28T15:00:50.631694Z",
     "shell.execute_reply": "2025-05-28T15:00:50.631081Z",
     "shell.execute_reply.started": "2025-05-28T15:00:50.298008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649cd7ec63324c22b8346263ae068e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7744f07de63543a587b2dab93e2709c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba39d511e7f4f53ae63ff05f67572db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "file_names_bacth.json:   0%|          | 0.00/724k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Define the dataset name and local directory\n",
    "main_dir = \"/kaggle/working/a\"\n",
    "up_dir = os.path.join(main_dir, \"abc\")\n",
    "\n",
    "os.makedirs(main_dir, exist_ok=True)\n",
    "os.makedirs(up_dir, exist_ok=True)\n",
    "\n",
    "repo_id = \"danhtran2mind/finetune-Real-ESRGAN-anime\"\n",
    "save_path = main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "snapshot_download(repo_id=repo_id, repo_type=\"dataset\", local_dir=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:00:57.375742Z",
     "iopub.status.busy": "2025-05-28T15:00:57.375109Z",
     "iopub.status.idle": "2025-05-28T15:00:57.380106Z",
     "shell.execute_reply": "2025-05-28T15:00:57.379197Z",
     "shell.execute_reply.started": "2025-05-28T15:00:57.375719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To temporary Model hub\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import snapshot_download\n",
    "# Initialize API\n",
    "api = HfApi()\n",
    "\n",
    "def upload_hf_dataset(save_dir=\"/kaggle/working/a/abc\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)    \n",
    "    # Upload the folder to the repository root\n",
    "    api.upload_folder(\n",
    "        folder_path=save_dir,  # Local folder path\n",
    "        repo_id=\"danhtran2mind/finetune-Real-ESRGAN-anime\",\n",
    "        repo_type=\"dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:01:04.329143Z",
     "iopub.status.busy": "2025-05-28T15:01:04.328587Z",
     "iopub.status.idle": "2025-05-28T15:01:04.344042Z",
     "shell.execute_reply": "2025-05-28T15:01:04.343281Z",
     "shell.execute_reply.started": "2025-05-28T15:01:04.329111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_inference(device, model_name, input_path, output_dir, outscale, model_path):\n",
    "    \"\"\"Run Real-ESRGAN inference.\"\"\"\n",
    "    cmd = [\"python\", \"inference_realesrgan.py\", \"-n\", model_name, \"-i\", input_path, \n",
    "           \"-o\", output_dir, \"--outscale\", str(outscale), \"--model_path\", model_path]\n",
    "    if device == \"cpu\":\n",
    "        cmd.append(\"--fp32\")\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "def calculate_psnr(img1_path, img2_path):\n",
    "    \"\"\"Calculate PSNR between two images.\"\"\"\n",
    "    try:\n",
    "        img1, img2 = [np.array(Image.open(p).convert('RGB')) for p in [img1_path, img2_path]]\n",
    "        if img1.shape != img2.shape:\n",
    "            raise ValueError(\"Images must have same dimensions\")\n",
    "        mse = np.mean((img1 - img2) ** 2)\n",
    "        return float('inf') if mse == 0 else 20 * math.log10(255.0 / math.sqrt(mse))\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating PSNR for {img1_path} vs {img2_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_paths(base_name, paths, subfolder):\n",
    "    \"\"\"Generate image file paths.\"\"\"\n",
    "    input_img = f\"{subfolder}/{base_name}T1.png\"\n",
    "    raw_img = Path(paths['raw']) / f\"{base_name}.jpg\"\n",
    "    if not raw_img.exists():\n",
    "        raw_img = Path(paths['raw']) / f\"{base_name}.png\"\n",
    "    return input_img, str(raw_img), f\"{paths['original']}/{base_name}T1_out.png\", f\"{paths['finetune']}/{base_name}T1_out.png\"\n",
    "\n",
    "def create_subfolder(images, chunk_idx, chunk_size, paths):\n",
    "    \"\"\"Create a subfolder with a chunk of images copied from multiscale.\"\"\"\n",
    "    subfolder = Path(paths['inference']) / f\"chunk_{chunk_idx}\"\n",
    "    subfolder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for img_name in images:\n",
    "        src_img = Path(paths['multiscale']) / f\"{img_name}T1.png\"\n",
    "        if src_img.exists():\n",
    "            shutil.copy(src_img, subfolder / f\"{img_name}T1.png\")\n",
    "    \n",
    "    return str(subfolder)\n",
    "\n",
    "def clear_inference_test_dir(inference_path):\n",
    "    \"\"\"Clear the inference test directory.\"\"\"\n",
    "    if os.path.exists(inference_path):\n",
    "        shutil.rmtree(inference_path)\n",
    "    os.makedirs(inference_path)\n",
    "    \n",
    "def save_to_json(original_psnr, finetuned_psnr, no_order,\n",
    "                 or_dir=\"/kaggle/working/a/abc\"):\n",
    "    data = {\"original_psnr\": original_psnr,\n",
    "            \"finetuned_psnr\": finetuned_psnr}\n",
    "    \n",
    "    pns_data_dir = os.path.join(or_dir, str(no_order))\n",
    "    os.makedirs(pns_data_dir, exist_ok=True)\n",
    "    pns_data_file = os.path.join(pns_data_dir, f\"psnr_data_{no_order}.json\")\n",
    "    \n",
    "    with open(pns_data_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def process_images(device, base_names, paths, model_configs, chunk_size, no_order, completed_indexes):\n",
    "    \"\"\"Process images in chunks by creating subfolders and calculate PSNR values.\"\"\"\n",
    "    \n",
    "    chunk_file_names = []  # To store file names for each chunk\n",
    "    \n",
    "    # Split base_names into chunks\n",
    "    for chunk_idx in tqdm(range(0, len(base_names), chunk_size), desc=\"Processing chunks\"):\n",
    "        original_psnr, finetuned_psnr = [], []\n",
    "        if chunk_idx in completed_indexes * chunk_size:\n",
    "            print(\"Pass processed state\")\n",
    "            no_order += 1\n",
    "            continue\n",
    "        print(f\"Do {chunk_idx}\")\n",
    "        chunk = base_names[chunk_idx:chunk_idx + chunk_size]\n",
    "        \n",
    "        # Store file names for this chunk\n",
    "        chunk_file_names.append(chunk)\n",
    "        \n",
    "        # Create a subfolder for this chunk\n",
    "        subfolder = create_subfolder(chunk, chunk_idx // chunk_size, chunk_size, paths)\n",
    "        \n",
    "        # Run inference on the subfolder\n",
    "        for model_type, model_path in model_configs:\n",
    "            run_inference(device, \"RealESRGAN_x4plus\", subfolder, paths[model_type], 2, model_path)\n",
    "        \n",
    "        # Process each image in the chunk\n",
    "        for base_name in chunk:\n",
    "            input_img, raw_img, original_img, finetuned_img = get_image_paths(base_name, paths, subfolder)\n",
    "            \n",
    "            if not os.path.exists(input_img):\n",
    "                print(f\"Input image not found: {input_img}\")\n",
    "                continue\n",
    "            \n",
    "            if all(os.path.exists(p) for p in [raw_img, original_img, finetuned_img]):\n",
    "                for img, psnr_list, model_type in [(original_img, original_psnr, \"Original\"), \n",
    "                                                 (finetuned_img, finetuned_psnr, \"Finetuned\")]:\n",
    "                    psnr = calculate_psnr(raw_img, img)\n",
    "                    if psnr is not None:\n",
    "                        psnr_list.append(psnr)\n",
    "                        # print(f\"{model_type} Model PSNR for {base_name}: {psnr:.4f}\")\n",
    "        \n",
    "        save_to_json(original_psnr, finetuned_psnr, no_order)\n",
    "        upload_hf_dataset()\n",
    "        # Clear the subfolder and temp inference after processing\n",
    "        clear_inference_test_dir(paths[\"inference\"])\n",
    "        clear_inference_test_dir(subfolder)\n",
    "        \n",
    "        no_order += 1\n",
    "        \n",
    "    return original_psnr, finetuned_psnr, chunk_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:01:10.032532Z",
     "iopub.status.busy": "2025-05-28T15:01:10.031965Z",
     "iopub.status.idle": "2025-05-28T15:01:10.085871Z",
     "shell.execute_reply": "2025-05-28T15:01:10.085094Z",
     "shell.execute_reply.started": "2025-05-28T15:01:10.032507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_CONFIGS = [\n",
    "    (\"original\", \"./experiments/pretrained_models/RealESRGAN_x4plus.pth\"),\n",
    "    (\"finetune\", \"./experiments/finetune_RealESRGAN_anime/models/net_g_latest.pth\")\n",
    "]\n",
    "PATHS = {\n",
    "    \"raw\": \"/kaggle/input/anime-images-raw\",\n",
    "    \"multiscale\": \"/kaggle/input/anime-images-multiscale\",\n",
    "    \"original\": \"/content/inference_test/original\",\n",
    "    \"finetune\": \"/content/inference_test/my_finetune\",\n",
    "    \"inference\": \"/content/inference_test\"\n",
    "}\n",
    "CHUNK_SIZE = 1000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Get image files\n",
    "# files = [f for f in os.listdir(PATHS[\"raw\"]) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "# base_names = [os.path.splitext(f)[0] for f in (files[:total_images] if 'total_images' in globals() else files)]\n",
    "\n",
    "# # Run PSNR extraction\n",
    "# original_psnr, finetuned_psnr, chunk_file_names = process_images(device, base_names, PATHS, MODEL_CONFIGS, CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:01:13.950684Z",
     "iopub.status.busy": "2025-05-28T15:01:13.950422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "worker = 0\n",
    "on_time = 7\n",
    "completed_indexes = []\n",
    "no_order = worker * on_time\n",
    "psnr_data = f\"/content/abc/psnr_data_{worker}.json\"\n",
    "\n",
    "with open('/kaggle/working/a/file_names_bacth.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "base_names = [name for i in range(on_time) for name in data[str(worker * on_time + i)]]\n",
    "\n",
    "original_psnr, finetuned_psnr, _ = process_images(device, base_names, PATHS,\n",
    "                                        MODEL_CONFIGS, CHUNK_SIZE, no_order, completed_indexes)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7464686,
     "sourceId": 11877691,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7465191,
     "sourceId": 11878510,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242018527,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242294281,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
